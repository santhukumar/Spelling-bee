{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Flatten, Dense, Input, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional, RepeatVector, TimeDistributed, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from io import open\n",
    "from keras.callbacks import EarlyStopping, ProgbarLogger\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import numpy\n",
    "from bidict import bidict\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Flatten, Dense, Input, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional, RepeatVector, TimeDistributed, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from io import open\n",
    "from keras.callbacks import EarlyStopping, ProgbarLogger\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "from attention import Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten= lambda l:[item for sublist in l for item in sublist]\n",
    "def randomly(seq):\n",
    "    shuffled = list(seq)\n",
    "    random.shuffle(shuffled)\n",
    "    return iter(shuffled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cmudict.0.6d') as f:\n",
    "    lines = [l.strip().split(\"  \") for l in  f.readlines()]\n",
    "    lines = [(l[0], l[1].split()) for l in lines if len(l)==2  and re.match(\"^[A-Z]+$\", l[0]) and  len(l[0])<16]\n",
    "\n",
    "    phonems = ['-'] + sorted(set(flatten([phs for w,phs in lines]))) + ['*']\n",
    "    letters = ['_'] + sorted(set(flatten([w for w, phs in lines]))) + ['*']\n",
    "    input_vocab_size = len(phonems)\n",
    "    output_vocab_size = len(letters)\n",
    "\n",
    "    char_vocab = dict(zip(letters, range(len(letters))))\n",
    "    phone_vocab = dict(zip(phonems, range(len(phonems))))\n",
    "\n",
    "    maxw_len = max([len(l[0]) for l in lines])\n",
    "    maxphs_len = max([len(l[1]) for l in lines])\n",
    "\n",
    "    X = np.zeros((len(lines), maxphs_len), np.int32)\n",
    "    Y = np.zeros((len(lines), maxw_len), np.int32)\n",
    "\n",
    "    for i, l in enumerate(randomly(lines)):\n",
    "        for j, ph in enumerate(l[1]): X[i][j] = phone_vocab[ph]\n",
    "        for j, ch in enumerate(l[0]): Y[i][j] = char_vocab[ch]\n",
    "\n",
    "    go_token = char_vocab[\"*\"]\n",
    "    dec_input_ = np.concatenate([np.ones((len(lines),1)) * go_token, Y[:,:-1]], axis=1)\n",
    "\n",
    "    X_train, X_test, X_d_train, X_d_test, y_train, y_test = train_test_split(X, dec_input_, Y, test_size=0.1)\n",
    "    #X_train, X_val, X_d_train, X_d_val, y_train, y_val = train_test_split(X_train, X_d_train, y_train, test_size=float(1)/9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 120\n",
    "\n",
    "def lstm_(dec_dim = EMB_SIZE, return_sequences= True): \n",
    "    return LSTM(2*dec_dim, dropout_U= 0.1, dropout_W= 0.1, consume_less= 'gpu', return_sequences=return_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((maxphs_len,))\n",
    "dec_i = Input((maxw_len,))\n",
    "dec_e = Embedding(output_vocab_size, EMB_SIZE)(dec_i)\n",
    "dec_e = Dense(2 * EMB_SIZE)(dec_e)\n",
    "\n",
    "x = Embedding(input_vocab_size, EMB_SIZE)(inp)\n",
    "x = Bidirectional(lstm_())(x)\n",
    "x = lstm_()(x)\n",
    "x = lstm_()(x)\n",
    "x = Attention(lstm_, 3)([x, dec_e])\n",
    "x = TimeDistributed(Dense(output_vocab_size, activation='softmax'))(x)\n",
    "model = Model([inp, dec_i], x)\n",
    "\n",
    "model.compile(Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100740 samples, validate on 2798 samples\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "100740/100740 [==============================] - 323s - loss: 0.1715 - acc: 0.9420 - val_loss: 0.1790 - val_acc: 0.9416\n",
      "100740/100740 [==============================] - 323s - loss: 0.1715 - acc: 0.9420 - val_loss: 0.1790 - val_acc: 0.9416\n",
      "Epoch 2/5\n",
      "Epoch 2/5\n",
      "100740/100740 [==============================] - 322s - loss: 0.1785 - acc: 0.9401 - val_loss: 0.1734 - val_acc: 0.9424\n",
      "100740/100740 [==============================] - 322s - loss: 0.1785 - acc: 0.9401 - val_loss: 0.1734 - val_acc: 0.9424\n",
      "Epoch 3/5\n",
      "Epoch 3/5\n",
      "100740/100740 [==============================] - 322s - loss: 0.1843 - acc: 0.9387 - val_loss: 0.2053 - val_acc: 0.9320\n",
      "100740/100740 [==============================] - 322s - loss: 0.1843 - acc: 0.9387 - val_loss: 0.2053 - val_acc: 0.9320\n",
      "Epoch 4/5\n",
      "Epoch 4/5\n",
      "100740/100740 [==============================] - 321s - loss: 0.1802 - acc: 0.9399 - val_loss: 0.1728 - val_acc: 0.9435\n",
      "100740/100740 [==============================] - 321s - loss: 0.1802 - acc: 0.9399 - val_loss: 0.1728 - val_acc: 0.9435\n",
      "Epoch 5/5\n",
      "Epoch 5/5\n",
      "100740/100740 [==============================] - 322s - loss: 0.1730 - acc: 0.9418 - val_loss: 0.1693 - val_acc: 0.9439\n",
      "100740/100740 [==============================] - 322s - loss: 0.1730 - acc: 0.9418 - val_loss: 0.1693 - val_acc: 0.9439\n",
      "Train on 100740 samples, validate on 2798 samples\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "100740/100740 [==============================] - 320s - loss: 0.1653 - acc: 0.9440 - val_loss: 0.1646 - val_acc: 0.9444\n",
      "100740/100740 [==============================] - 320s - loss: 0.1653 - acc: 0.9440 - val_loss: 0.1646 - val_acc: 0.9444\n",
      "Epoch 2/5\n",
      "Epoch 2/5\n",
      "100740/100740 [==============================] - 322s - loss: 0.1625 - acc: 0.9452 - val_loss: 0.1642 - val_acc: 0.9446\n",
      "100740/100740 [==============================] - 322s - loss: 0.1625 - acc: 0.9452 - val_loss: 0.1642 - val_acc: 0.9446\n",
      "Epoch 3/5\n",
      "Epoch 3/5\n",
      "100740/100740 [==============================] - 320s - loss: 0.1615 - acc: 0.9454 - val_loss: 0.1733 - val_acc: 0.9441\n",
      "100740/100740 [==============================] - 320s - loss: 0.1615 - acc: 0.9454 - val_loss: 0.1733 - val_acc: 0.9441\n",
      "Epoch 4/5\n",
      "Epoch 4/5\n",
      "100740/100740 [==============================] - 319s - loss: 0.1664 - acc: 0.9439 - val_loss: 0.1665 - val_acc: 0.9457\n",
      "100740/100740 [==============================] - 319s - loss: 0.1664 - acc: 0.9439 - val_loss: 0.1665 - val_acc: 0.9457\n",
      "Epoch 5/5\n",
      "Epoch 5/5\n",
      "100740/100740 [==============================] - 320s - loss: 0.1742 - acc: 0.9418 - val_loss: 0.1792 - val_acc: 0.9413\n",
      "100740/100740 [==============================] - 320s - loss: 0.1742 - acc: 0.9418 - val_loss: 0.1792 - val_acc: 0.9413\n",
      "Train on 100740 samples, validate on 2798 samples\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "100740/100740 [==============================] - 320s - loss: 0.1656 - acc: 0.9443 - val_loss: 0.1601 - val_acc: 0.9462\n",
      "100740/100740 [==============================] - 320s - loss: 0.1656 - acc: 0.9443 - val_loss: 0.1601 - val_acc: 0.9462\n",
      "Epoch 2/5\n",
      "Epoch 2/5\n",
      " 40960/100740 [===========>..................] - ETA: 188s - loss: 0.1585 - acc: 0.9466"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.fit([X_train, X_d_train], np.expand_dims(y_train,-1), validation_data=[[X_test, X_d_test], np.expand_dims(y_test,-1)], batch_size=64, verbose=1,callbacks= [ProgbarLogger()], nb_epoch=5)\n",
    "    model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('validation accuracy', 0.44281629735525374)\n"
     ]
    }
   ],
   "source": [
    "new_l = len(X_test)/4\n",
    "X_test, X_d_test, y_test = X_test[:new_l], X_d_test[:new_l], y_test[:new_l]\n",
    "def eval_keras():\n",
    "    preds = model.predict([X_test, X_d_test], batch_size=128)\n",
    "    predict = np.argmax(preds, axis = 2)\n",
    "    return (np.mean([all(real==p) for real, p in zip(y_test, predict)]), predict)\n",
    "\n",
    "\n",
    "acc, preds = eval_keras(); \n",
    "print('validation accuracy', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pronunciation                           ', 'real spelling    ', 'model spelling   ', 'is correct')\n",
      "(u'S-IH1-M-AH0-N-OW0                       ', u'SIMONEAU         ', u'SIMINOAU         ', 'False')\n",
      "(u'D-IH0-L-EH1-L-OW0                       ', u'DILELLO          ', u'DELELLO          ', 'False')\n",
      "(u'SH-EH2-V-R-AH0-L-EY1                    ', u'CHEVROLET        ', u'SHEVRALAT        ', 'False')\n",
      "(u'V-AE2-N-HH-IY1-L                        ', u'VANHEEL          ', u'VANHEAL          ', 'False')\n",
      "(u'P-EY1-Z-ER0                             ', u'PEYSER           ', u'PAISOR           ', 'False')\n",
      "(u'F-R-AE2-N-CH-AY0-Z-IY1-Z                ', u'FRANCHISEES      ', u'FRANCHISESS      ', 'False')\n",
      "(u'EH1-S-K-ER0                             ', u'ESKER            ', u'ESKER            ', 'True')\n",
      "(u'P-AH1-B                                 ', u'PUB              ', u'PUBB             ', 'False')\n",
      "(u'Z-IY1-W-EY0                             ', u'ZIWEI            ', u'ZEWA             ', 'False')\n",
      "(u'R-AY1-M                                 ', u'RHYME            ', u'REIME            ', 'False')\n",
      "(u'D-EH0-L-UW1-CH-AH0                      ', u'DELUCCIA         ', u'DELUCCIA         ', 'True')\n",
      "(u'L-AO1-R                                 ', u'LOEHR            ', u'LORRR            ', 'False')\n",
      "(u'R-AA1-T-IH0-D                           ', u'ROTTED           ', u'ROTTED           ', 'True')\n",
      "(u'P-IY1-S-F-AH0-L-IY0                     ', u'PEACEFULLY       ', u'PEASEFULLY       ', 'False')\n",
      "(u'S-AE1-N-D-S-T-AO2-R-M                   ', u'SANDSTORM        ', u'SANDSTORM        ', 'True')\n",
      "(u'K-UW1-B-AH0                             ', u'KOUBA            ', u'KUUBA            ', 'False')\n",
      "(u'B-IY1-K                                 ', u'BEAK             ', u'BEEK             ', 'False')\n",
      "(u'R-IH0-G-EY1-N                           ', u'REGAIN           ', u'REGAIN           ', 'True')\n",
      "(u'D-AH1-N-B-AA0-R                         ', u'DUNBAR           ', u'DUNBAR           ', 'True')\n",
      "(u'AA0-T-S-AA1-R-AH0                       ', u'AZZARA           ', u'AZZARA           ', 'True')\n"
     ]
    }
   ],
   "source": [
    "print(\"pronunciation\".ljust(40), \"real spelling\".ljust(17), \n",
    "      \"model spelling\".ljust(17), \"is correct\")\n",
    "\n",
    "for index in range(20):\n",
    "    ps = \"-\".join([phonems[p] for p in X_test[index]]) \n",
    "    real = [letters[l] for l in y_test[index]] \n",
    "    predict = [letters[l] for l in preds[index]]\n",
    "    print (ps.split(\"--\")[0].ljust(40), \"\".join(real).split(\"_\")[0].ljust(17),\n",
    "        \"\".join(predict).split(\"_\")[0].ljust(17), str(real == predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook spelling bee.ipynb to script\n",
      "[NbConvertApp] Writing 4731 bytes to spelling bee.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script spelling\\ bee.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
