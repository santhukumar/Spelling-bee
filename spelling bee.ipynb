{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Flatten, Dense, Input, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional, RepeatVector, TimeDistributed, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from io import open\n",
    "from keras.callbacks import EarlyStopping, ProgbarLogger\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import numpy\n",
    "from bidict import bidict\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Flatten, Dense, Input, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional, RepeatVector, TimeDistributed, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from io import open\n",
    "from keras.callbacks import EarlyStopping, ProgbarLogger\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "from attention import Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten= lambda l:[item for sublist in l for item in sublist]\n",
    "def randomly(seq):\n",
    "    shuffled = list(seq)\n",
    "    random.shuffle(shuffled)\n",
    "    return iter(shuffled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cmudict.0.6d') as f:\n",
    "    lines = [l.strip().split(\"  \") for l in  f.readlines()]\n",
    "    lines = [(l[0], l[1].split()) for l in lines if len(l)==2  and re.match(\"^[A-Z]+$\", l[0]) and  len(l[0])<9]\n",
    "\n",
    "    phonems = ['-'] + sorted(set(flatten([phs for w,phs in lines]))) + ['*']\n",
    "    letters = ['_'] + sorted(set(flatten([w for w, phs in lines]))) + ['*']\n",
    "    input_vocab_size = len(phonems)\n",
    "    output_vocab_size = len(letters)\n",
    "\n",
    "    char_vocab = dict(zip(letters, range(len(letters))))\n",
    "    phone_vocab = dict(zip(phonems, range(len(phonems))))\n",
    "\n",
    "    maxw_len = max([len(l[0]) for l in lines])\n",
    "    maxphs_len = max([len(l[1]) for l in lines])\n",
    "\n",
    "    X = np.zeros((len(lines), maxphs_len), np.int32)\n",
    "    Y = np.zeros((len(lines), maxw_len), np.int32)\n",
    "\n",
    "    for i, l in enumerate(randomly(lines)):\n",
    "        for j, ph in enumerate(l[1]): X[i][j] = phone_vocab[ph]\n",
    "        for j, ch in enumerate(l[0]): Y[i][j] = char_vocab[ch]\n",
    "\n",
    "    go_token = char_vocab[\"*\"]\n",
    "    dec_input_ = np.concatenate([np.ones((len(lines),1)) * go_token, Y[:,:-1]], axis=1)\n",
    "\n",
    "    X_train, X_test, X_d_train, X_d_test, y_train, y_test = train_test_split(X, dec_input_, Y, test_size=0.1)\n",
    "    #X_train, X_val, X_d_train, X_d_val, y_train, y_val = train_test_split(X_train, X_d_train, y_train, test_size=float(1)/9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 120\n",
    "\n",
    "def lstm_(dec_dim = EMB_SIZE, return_sequences= True): \n",
    "    return LSTM(2*dec_dim, dropout_U= 0.1, dropout_W= 0.1, consume_less= 'gpu', return_sequences=return_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((maxphs_len,))\n",
    "dec_i = Input((maxw_len,))\n",
    "dec_e = Embedding(output_vocab_size, EMB_SIZE)(dec_i)\n",
    "dec_e = Dense(2 * EMB_SIZE)(dec_e)\n",
    "\n",
    "x = Embedding(input_vocab_size, EMB_SIZE)(inp)\n",
    "x = Bidirectional(lstm_())(x)\n",
    "x = lstm_()(x)\n",
    "x = lstm_()(x)\n",
    "x = Attention(lstm_, 3)([x, dec_e])\n",
    "x = TimeDistributed(Dense(output_vocab_size, activation='softmax'))(x)\n",
    "model = Model([inp, dec_i], x)\n",
    "\n",
    "model.compile(Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73315 samples, validate on 8147 samples\n",
      "Epoch 1/6\n",
      "Epoch 1/6\n",
      "40832/73315 [===============>..............] - ETA: 611s - loss: 0.9093 - acc: 0.6947"
     ]
    }
   ],
   "source": [
    "model.fit([X_train, X_d_train], np.expand_dims(y_train,-1), validation_data=[[X_test, X_d_test], np.expand_dims(y_test,-1)], batch_size=64, verbose=1,callbacks= [ProgbarLogger()], nb_epoch=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_keras():\n",
    "    preds = model.predict([X_train, X_d_train], batch_size=128)\n",
    "    predict = np.argmax(preds, axis = 2)\n",
    "    return (np.mean([all(real==p) for real, p in zip(y_train, predict)]), predict)\n",
    "\n",
    "\n",
    "acc, preds = eval_keras(); acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pronunciation                           ', 'real spelling    ', 'model spelling   ', 'is correct')\n",
      "(u'L-AO1-NG-G-OW0------------              ', u'LONGO            ', u'LANGE            ', 'False')\n",
      "(u'Z-EH1-L-ER0-Z------------               ', u'ZELLERS          ', u'ZELLERS          ', 'True')\n",
      "(u'F-R-IH1-K-ER0------------               ', u'FRICKER          ', u'FRICKER          ', 'True')\n",
      "(u'B-AA2-R-D-EH1-R-AH0--------             ', u'BARDERA          ', u'BORDERY          ', 'False')\n",
      "(u'K-ER1-N-AH0-L------------               ', u'KERNEL           ', u'CARNELL          ', 'False')\n",
      "(u'R-OW1-L-F-S------------                 ', u'ROHLFS           ', u'ROLLES           ', 'False')\n",
      "(u'SH-ER1-G-ER0--------------              ', u'SCHERGER         ', u'SHHAAEER         ', 'False')\n",
      "(u'Y-EY1-Z----------------                 ', u'YEAS             ', u'EESS             ', 'False')\n",
      "(u'F-EH1-S-T-AH0-S----------               ', u'FESTUS           ', u'FETTSS           ', 'False')\n",
      "(u'HH-AE1-Z----------------                ', u'HAS              ', u'HASS             ', 'False')\n",
      "(u'S-L-AA1-M-AH0------------               ', u'SLAMA            ', u'GLAMM            ', 'False')\n",
      "(u'G-IH1-L-B--------------                 ', u'GILB             ', u'GORB             ', 'False')\n",
      "(u'K-AA1-P-AH0-L-AH0-N--------             ', u'COPLEN           ', u'COPAIN           ', 'False')\n",
      "(u'TH-AY1-AH0-M-AH0-N----------            ', u'THIAMIN          ', u'THADDIN          ', 'False')\n",
      "(u'N-OY1-Z----------------                 ', u'NOISE            ', u'NODS             ', 'False')\n",
      "(u'R-AA1-S-AO0-F------------               ', u'ROSOFF           ', u'ROSSLF           ', 'False')\n",
      "(u'W-EH1-SH----------------                ', u'WESCH            ', u'WISHH            ', 'False')\n",
      "(u'M-IY1-N-IY0--------------               ', u'MEANEY           ', u'MANNE            ', 'False')\n",
      "(u'L-AO1-M-EH1-N------------               ', u'LAWMEN           ', u'LAMMAN           ', 'False')\n",
      "(u'P-UW1-L-AH0-NG-K----------              ', u'POULENC          ', u'POLLINSE         ', 'False')\n"
     ]
    }
   ],
   "source": [
    "print(\"pronunciation\".ljust(40), \"real spelling\".ljust(17), \n",
    "      \"model spelling\".ljust(17), \"is correct\")\n",
    "\n",
    "for index in range(20):\n",
    "    ps = \"-\".join([phonems[p] for p in X_train[index]]) \n",
    "    real = [letters[l] for l in y_train[index]] \n",
    "    predict = [letters[l] for l in preds[index]]\n",
    "    print (ps.split(\"-_\")[0].ljust(40), \"\".join(real).split(\"_\")[0].ljust(17),\n",
    "        \"\".join(predict).split(\"_\")[0].ljust(17), str(real == predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
